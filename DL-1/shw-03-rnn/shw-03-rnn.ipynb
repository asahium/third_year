{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c368f2",
   "metadata": {},
   "source": [
    "# Глубинное обучение 1 / Введение в глубинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 3: RNN и языковые модели \n",
    "\n",
    "### Общая информация\n",
    "\n",
    "Оценка после штрафа после мягкого дедлайна вычисляется по формуле $M_{\\text{penalty}} = M_{\\text{full}} \\cdot 0.85^{t/1440}$, где $M_{\\text{full}}$ — полная оценка за работу без учета штрафа, а $t$ — время в минутах, прошедшее после мягкого дедлайна (округление до двух цифр после запятой). Таким образом, спустя первые сутки после мягкого дедлайна вы не можете получить оценку выше 12.75, а если сдать через четыре дня после мягкого дедлайна, то ваш максимум — 7.83 балла.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 15 баллов. Сдавать задание после указанного срока сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит обучить рекуррентную нейронную сеть для задачи генерации текстов. В качестве данных возьмем набор из 120 тысяч анекдотов (всех категорий от А до Я включительно). Его вы можете найти в архиве `jokes.txt.zip`, который доступен по [ссылке](https://disk.yandex.com/d/fjt5xICH-ukEEA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b35b1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daae037",
   "metadata": {},
   "source": [
    "## Задание 1: Dataset (1 балл)\n",
    "\n",
    "В этом задании мы будет пользоваться библиотекой [sentencepiece](https://github.com/google/sentencepiece), которая поддерживает разные форматы токенизации текстов, в том числе BPE, который мы и будем использовать. Реализуйте недостающие фрагменты кода в классе `TextDataset` в файле `dataset.py`. Датасет обучает sentencepiece токенизатор, токенизирует тексты, превращает токены в индексы и паддит до одной и той же длины (параметр `max_length`). Не забудьте, что для генерации текстов нам будут нужны специальные токены начала и конца последовательности, соответственно `BOS` и `EOS`. Существуют еще два специальных токена &mdash; паддинг `PAD` и токен `UNK`, заменяющий out-of-vocabulary токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dfa4648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/wyrm/anaconda3/lib/python3.9/site-packages (0.1.97)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ed11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from dataset import TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81630e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TextDataset(data_file='jokes.txt', train=True, sp_model_prefix='bpe')\n",
    "valid_set = TextDataset(data_file='jokes.txt', train=False, sp_model_prefix='bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27555b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код должен проходить тесты\n",
    "assert len(train_set) + len(valid_set) == 120759\n",
    "\n",
    "for _ in range(5):\n",
    "    for dataset in (train_set, valid_set):\n",
    "        indices, length = dataset[np.random.randint(len(dataset))]\n",
    "        assert indices.shape == (dataset.max_length, )\n",
    "        assert indices[0].item() == dataset.bos_id\n",
    "        assert (indices == dataset.eos_id).sum().item() == 1\n",
    "\n",
    "        eos_pos = indices.tolist().index(dataset.eos_id)\n",
    "        assert torch.all(indices[eos_pos + 1:] == dataset.pad_id)\n",
    "        assert (indices != dataset.pad_id).sum() == length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db087d5",
   "metadata": {},
   "source": [
    "## Задание 2 Language model (3.5 балла)\n",
    "\n",
    "Реализуйте класс `LanguageModel` из файла `model.py`. Мы будем генерировать текст с помощью языковой модели &mdash; это авторегрессионная вероятностная модель, которая предсказывает распределение следующего токена при условии предыдущих:\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, x_3, \\dots, x_T) = p(x_1) \\cdot p(x_2 | x_1) \\cdot p(x_3|x_1, x_2) \\, \\cdot \\, \\dots \\, \\cdot \\, p(x_T|x_1, \\dots, x_{T-1})\n",
    "$$\n",
    "\n",
    "Мы будем реализовывать ее с помощью рекуррентной нейронной сети. Ваш код должен поддерживать возможность работать как с оригинальной [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN), так и c [LSTM](https://bitly.com/98K8eH). На каждом временном шаге модель возвращает логиты вероятностей для следующего токена. Модель будет работать в двух режимах (не путать с `.train()` и `.eval()`):\n",
    "\n",
    "- В режиме обучения (метод `forward`) модель принимает настоящие последовательности из датасета и их длины. На каждом временном шаге возвращаются логиты вероятностей следующего токена, что позволяет считать лосс, обучаться на трейне и валидироваться на валидации.\n",
    "\n",
    "- В режиме генерации (инференса, метод `inference`) модель принимает некоторый префикс (возможно пустой), с которого начинать генерацию, и продолжает его. Для этого на каждом шаге генерируются новые логиты, семплируется новый токен (из распределения, заданного логитами), и процесс продолжается, пока не будет сгенерирован токен `UNK` или не будет достигнуто ограничение на длину последовательности. **Обратите внимание**, что вам не нужно прогонять всю последовательность заново через RNN после каждого нового токена, это приведет к квадратичной сложности по длине последовательности. Вам достаточно обновлять скрытое состояние, подавая на вход новый сгенерированный токен и предыдущее скрытое состояние. Кроме того, чтобы получить больше контроля над генерацией, вводится параметр температуры `temp`. Перед семплированием нужно разделить на него логиты, полученные моделью. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378c1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a18e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99427388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(train_set, rnn_layers=1, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199406f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Код должен проходить тесты\n",
    "for bs in [1, 4, 16, 64, 256]:\n",
    "    indices = torch.randint(high=train_set.vocab_size, size=(bs, train_set.max_length)).to(device)\n",
    "    lengths = torch.randint(low=1, high=train_set.max_length + 1, size=(bs, ))\n",
    "    logits = model(indices, lengths)\n",
    "    assert logits.shape == (bs, lengths.max(), train_set.vocab_size)\n",
    "\n",
    "for prefix in ['', 'купил мужик шляпу,', 'сел медведь в машину и', 'подумал штирлиц']:\n",
    "    generated = model.inference(prefix, temp=np.random.uniform(0.1, 10))\n",
    "    assert type(generated) == str\n",
    "    assert generated.startswith(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eacf83",
   "metadata": {},
   "source": [
    "## Задание 3: Training (2 балла)\n",
    "\n",
    "Всё, что нам осталось &mdash; реализовать цикл обучения. Заполните пропуски в файле `train.py`. Не забудьте, что мы учим модель предсказывать вероятность следующего, а не текущего токена. Также рекомендуется обрезать батч индексов по самой длинной последовательности, чтобы не гонять паддинги вхолостую. Для оценки качества генерации будем использовать метрику [perplexity](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94). Реализуйте ее подсчет в функции `plot_losses` (да, для этого достаточно только значения лосса).\n",
    "\n",
    "Обучите модель, используя ванильную RNN в качестве рекуррентного слоя. Сохраните чекпойнт обученной модели, он нам еще пригодится. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be269b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54bac4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = None #torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs = 5\n",
    "num_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f40fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff937c3fc1fc47a08b41310be4e2067e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training 1/5:   0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/wyrm/Documents/GitHub/third_year/DL-1/shw-03-rnn/shw-03-rnn.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wyrm/Documents/GitHub/third_year/DL-1/shw-03-rnn/shw-03-rnn.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(lang_model, optimizer, scheduler, train_loader, val_loader, num_epochs, num_examples)\n",
      "File \u001b[0;32m~/Documents/GitHub/third_year/DL-1/shw-03-rnn/train.py:134\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_loader, val_loader, num_epochs, num_examples)\u001b[0m\n\u001b[1;32m    131\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[39m=\u001b[39mtrain_loader\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mpad_id)\n\u001b[1;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 134\u001b[0m     train_loss \u001b[39m=\u001b[39m training_epoch(\n\u001b[1;32m    135\u001b[0m         model, optimizer, criterion, train_loader,\n\u001b[1;32m    136\u001b[0m         tqdm_desc\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTraining \u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mnum_epochs\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m     val_loss \u001b[39m=\u001b[39m validation_epoch(\n\u001b[1;32m    139\u001b[0m         model, criterion, val_loader,\n\u001b[1;32m    140\u001b[0m         tqdm_desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidating \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/third_year/DL-1/shw-03-rnn/train.py:73\u001b[0m, in \u001b[0;36mtraining_epoch\u001b[0;34m(model, optimizer, criterion, loader, tqdm_desc)\u001b[0m\n\u001b[1;32m     70\u001b[0m indices \u001b[39m=\u001b[39m indices[:, :lengths\u001b[39m.\u001b[39mmax()]\n\u001b[1;32m     71\u001b[0m indices \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mlong()\n\u001b[0;32m---> 73\u001b[0m logits \u001b[39m=\u001b[39m model(indices[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], lengths \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     74\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), indices[:, \u001b[39m1\u001b[39m:])\n\u001b[1;32m     76\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m indices\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/third_year/DL-1/shw-03-rnn/model.py:43\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[0;34m(self, indices, lengths)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, indices: torch\u001b[39m.\u001b[39mTensor, lengths: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     36\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m    Compute forward pass through the model and\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m    return logits for the next token probabilities\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m    :return: FloatTensor of logits of shape (batch_size, length, vocab_size)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(indices)        \n\u001b[1;32m     44\u001b[0m     packed_embeddings \u001b[39m=\u001b[39m pack_padded_sequence(embeddings, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m     output_packed, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(packed_embeddings)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_loader, val_loader, num_epochs, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'rnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dad5e1",
   "metadata": {},
   "source": [
    "## Задание 4: LSTM (0.5 балла)\n",
    "\n",
    "Обучите аналогичную модель, но с LSTM в качестве рекуррентного слоя. Сравните модели по метрикам и генерации. Не забывайте про чекпойнты!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(train_set, rnn_type=nn.LSTM, rnn_layers=1, device=device).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = None #torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs = 5\n",
    "num_examples = 5\n",
    "\n",
    "train(model, optimizer, scheduler, train_loader, val_loader, num_epochs, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0168d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lstm.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23819c",
   "metadata": {},
   "source": [
    "## Задание 5: Sampling temperature (0.5 балла)\n",
    "\n",
    "Поэкспериментируйте, как результат генерации зависит от параметра температуры. Попробуйте генерацию с разными префиксами. Сделайте выводы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(train_set, rnn_type=nn.LSTM, rnn_layers=1, device=device).to(device)\n",
    "model.load_state_dict(torch.load('lstm.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = [0.1, 0.5, 1, 2, 5, 10]\n",
    "\n",
    "for temp in temp_list:\n",
    "    print(temp, model.inference('Путин', temp=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c14632",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in temp_list:\n",
    "    print(temp, model.inference('Купил мужик шляпу,', temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068ec33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in temp_list:\n",
    "    print(temp, model.inference('Сел медведь в машину и', temp=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in temp_list:\n",
    "    print(temp, model.inference('Подумал штирлиц', temp=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3967c7",
   "metadata": {},
   "source": [
    "## Задание 5: Tokenizers (1 балл)\n",
    "\n",
    "До сих пор мы использовали BPE токенизатор с относительно небольшим числом токенов (2000 по умолчанию). Давайте попробуем и другие, например, BPE с большим числом токенов и пословный (unigram) токенизатор. Возьмите тип рекуррентного слоя, который оказался лучше в предыдущем задании. Обучите модели на таких токенизаторах и сравните их генерацию. Не забывайте сохранять чекпойнты. Правильно ли сравнивать между собой получившиеся модели по значению perplexity? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "train_set = TextDataset(data_file='jokes.txt', train=True, vocab_size=vocab_size, sp_model_prefix='bpe_vocab_big')\n",
    "valid_set = TextDataset(data_file='jokes.txt', train=False, vocab_size=vocab_size, sp_model_prefix='bpe_vocab_big')\n",
    "\n",
    "model = LanguageModel(train_set, rnn_type=nn.LSTM, rnn_layers=1, device=device).to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "\n",
    "optimizer = torch.optim.Adam(lang_model.parameters(), lr=lr)\n",
    "scheduler = None #torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs = 5\n",
    "num_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocab size {vocab_size}\")\n",
    "train(model, optimizer, scheduler, train_loader, val_loader, num_epochs, num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc550928",
   "metadata": {},
   "source": [
    "## Задание 6. Latent Semantic Analysis (2 балла)\n",
    "\n",
    "Попробуем другой подход к оцениванию качества генерации, основанный на [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis). Реализуйте следующую метрику и сравните по ней модели, обученные с разными токенизаторами:\n",
    "\n",
    "1. Генерируем обученной моделью выборку текстов, совпадающую по размеру с валидационной выборкой.\n",
    "2. Объединяем две выборки текстов (валидационную и сгенерированную) в один корпус. Обратите внимание, что наша токенизация в общем случае необратима, поэтому для чистоты эксперимента нужно закодировать и декодировать валидационную выборку.\n",
    "3. Генерируем tf-idf матрицу для полученного корпуса.\n",
    "4. Понижаем размерность матрицы с помощью [SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
    "5. Теперь у нас есть векторы, описывающие валидационные и сгенерированные тексты, лежащие в одном пространстве. Для каждого вектора, отвечающего сгенерированному тексту, найдем наибольший cosine similarity между ним и вектором валидационного текста. Усредним такие similarity по всем сгенерированным текстам и получим число, характеризующее похожесть сгенерированной выборки на валидационную.\n",
    "\n",
    "Какие плюсы и минусы есть у описанной метрики?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9235ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e632af",
   "metadata": {},
   "source": [
    "## Задание 7. Visualization (1 балл)\n",
    "\n",
    "В прошлом пункте мы получили векторы, описывающие валидационные и сгенерированные тексты. Попробуем визуализировать их. Примените [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) к этим векторам и нарисуйте scatter-plot с получившимися двумерными представлениями. Точки, соответствующие валидационным и сгенерированным текстам, должны быть разного цвета. Визуализируйте таким образом все три модели для разных токенизаторов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aada1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9cba5",
   "metadata": {},
   "source": [
    "## Задание 8. ruGPT perplexity (3.5 балла)\n",
    "\n",
    "Подход Latent Semantic Analysis, как и многие другие классические методы, заметно уступает нейросетевым алгоритмам анализа текстов. Вернемся к оцениванию качества генерации с помощью perplexity, для этого возьмем большую и хорошо обученную языковую модель, которая училась на огромном корпусе русских текстов. Считается, что большие языковые модели хорошо выучивают естественный язык, потому с их помощью мы сможем оценивать качество наших маленьких моделей для генерации анекдотов. Для этого мы воспользуемся сервисом [HuggingFace](https://huggingface.co/), который содержит огромное число обученных моделей для самых разных задач. Изучите и реализуйте, [подсчет perplexity](https://huggingface.co/docs/transformers/perplexity), с использованием обученной языковой модели. В качестве модели возьмите [ruGPT3-small](https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2). Сгенерируйте синтетические выборки тремя моделями, обученными выше (можете взять выборки из задания 6), и сравните их по perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (⊃｡•́‿•̀｡)⊃━✿✿✿✿✿✿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5d3dc",
   "metadata": {},
   "source": [
    "## Бонус (0.1 балл)\n",
    "\n",
    "Покажите лучший анекдот, который удалось сгенерировать вашей модели. Если проверяющий найдет его смешным, то поставит 0.1 балла."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea54e9a06830d9f69ada42b6a49c7b78a17357d1b939325917a1cffa8b64385c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
